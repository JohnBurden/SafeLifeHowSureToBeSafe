#!/usr/bin/env python3

"""
Main entry point for starting a training job.
"""

import os
import sys
import argparse
import subprocess
import shutil
import logging
import logging.config

env_types = [
    'append-still',
    'append-spawn',
    'prune-still',
    'prune-spawn',
    'navigate',
    'append-still-all',
]

parser = argparse.ArgumentParser(description="""
    Run agent training using proximal policy optimization.

    This will set up the data/log directories, optionally install any needed
    dependencies, start tensorboard, configure loggers, and start the actual
    training loop. If the data directory already exists, it will prompt for
    whether the existing data should be overwritten or appended. The latter
    allows for training to be restarted if interrupted.
    """)
parser.add_argument('data_dir',
    help="the directory in which to store this run's data")
parser.add_argument('--shutdown', action="store_true",
    help="Shut down the system when the job is complete"
    "(helpful for running remotely).")
parser.add_argument('--port', default=6006, type=int,
    help="Port on which to run tensorboard.")
parser.add_argument('--run-benchmarks', action="store_true",
    help="Don't do training; just run benchmarks on all benchmark levels.")
parser.add_argument('--which-benchmark', default=None)
parser.add_argument('--impact-penalty', default=0.0, type=float)
parser.add_argument('--penalty-baseline',
    choices=('starting-state', 'inaction'), default='starting-state')
parser.add_argument('--env-type', choices=env_types)
parser.add_argument('--algo', choices=('ppo', 'dqn', 'ua', 'noop', 'wrappedDQN', 'wrappedPPO', 'wrappedUA', 'confidenceDQN', 'confidencePPO'), default='ppo')
parser.add_argument('--seed', default=None, type=int)
args = parser.parse_args()


# Setup the directories

safety_dir = os.path.realpath(os.path.join(__file__, '../'))
data_dir = os.path.realpath(args.data_dir)
job_name = os.path.basename(data_dir)
sys.path.insert(1, safety_dir)  # ensure current directory is on the path
os.chdir(safety_dir)

print(args.run_benchmarks)

if os.path.exists(data_dir) and args.data_dir is not None:
    print("The directory '%s' already exists. "
          "Would you like to overwrite the old data, append to it, or abort?" %
          data_dir)
    response = None
    response = 'append' if args.run_benchmarks else \
               'overwrite' if job_name.startswith('tmp') else None
    while response not in ('overwrite', 'append', 'abort'):
        response = input("(overwrite / append / abort) > ")
    if response == 'overwrite':
        print("Overwriting old data.")
        shutil.rmtree(data_dir)
    elif response == 'abort':
        print("Aborting.")
        exit()

os.makedirs(data_dir, exist_ok=True)
logfile = os.path.join(data_dir, 'training.log')


def getNum(x):
    return ''.join(ele for ele in x if ele.isdigit())

# Get the environment type from the job name if not otherwise supplied
if args.env_type:
    env_type = args.env_type
else:
    for env_type in env_types:
        if env_type in job_name:
            break
    else:
        env_type = 'prune-still'

assert env_type in env_types


# Setup logging

if not os.path.exists(logfile):
    open(logfile, 'w').close()  # write an empty file
logging_config = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'simple': {
            'format': '{levelname:8s} {message}',
            'style': '{',
        },
        'dated': {
            'format': '{asctime} {levelname} ({filename}:{lineno}) {message}',
            'style': '{',
            'datefmt': '%Y-%m-%d %H:%M:%S',
        },
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'level': 'INFO',
            'stream': 'ext://sys.stdout',
            'formatter': 'simple',
        },
        'logfile': {
            'class': 'logging.FileHandler',
            'level': 'INFO',
            'formatter': 'dated',
            'filename': logfile,
        }
    },
    'loggers': {
        'training': {
            'level': 'INFO',
            'propagate': False,
            'handlers': ['console', 'logfile'],
        },
        'safelife': {
            'level': 'INFO',
            'propagate': False,
            'handlers': ['console', 'logfile'],
        }
    },
    'root': {
        'level': 'WARNING',
        'handlers': ['console', 'logfile'],
    }
}
logging.config.dictConfig(logging_config)


# Build the safelife C extensions.
# By making the build lib the same as the base folder, the extension
# should just get built into the source directory.
subprocess.run([
    "python3", os.path.join(safety_dir, "setup.py"),
    "build_ext", "--build-lib", safety_dir
])


# Start tensorboard

if args.port:
    tb_proc = subprocess.Popen([
        "tensorboard", "--logdir_spec", job_name + ':' + data_dir, '--port', str(args.port)])


# Start training!

try:
    import numpy as np
    from training.env_factory import (
        LinearSchedule,
        SafeLifeLevelIterator,
        SwitchingLevelIterator,
        safelife_env_factory
    )
    from safelife.safelife_logger import SafeLifeLogger
    from safelife.random import set_rng

    logger = logging.getLogger('training')

    seed1, seed2 = np.random.SeedSequence(args.seed).spawn(2)
    logger.info("SETTING GLOBAL SEED: %i", seed1.entropy)
    set_rng(np.random.default_rng(seed1))

    for penalty in [args.impact_penalty]:
        subdir = os.path.join(data_dir, "penalty_{:0.2f}".format(penalty))
        os.makedirs(subdir, exist_ok=True)

        if args.run_benchmarks:
            data_logger = SafeLifeLogger(
                subdir,
                summary_writer=False,
                training_log=False,
                testing_video_name="benchmark-{level_name}",
                testing_log=getNum(args.which_benchmark)+'-benchmark-data.json')
        else:
            data_logger = SafeLifeLogger(subdir)

        if env_type == 'append-still':
            t_penalty = [1.0e6, 2.0e6]
            t_performance = [1.0e6, 2.0e6]
            level_iterator = SafeLifeLevelIterator(
                'random/append-still.yaml',
                seed=seed2
            )
            test_levels = 'benchmarks/v1.0/append-still.npz'

        if env_type == 'append-still-all':
            t_penalty = [1.0e6, 2.0e6]
            t_performance = [1.0e6, 2.0e6]
            level_iterator = SafeLifeLevelIterator(
                'random/append-still-all.yaml',
                seed=seed2
            )
            test_levels = 'benchmarks/v1.0/append-still.npz'


        elif env_type == 'prune-still':
            t_penalty = [0.5e6, 1.5e6]
            t_performance = [0.5e6, 1.5e6]
            level_iterator = SafeLifeLevelIterator(
                'random/prune-still-easy.yaml',
                seed=seed2
            )
            test_levels = 'benchmarks/v1.0/prune-still.npz'

        elif env_type == 'append-spawn':
            # When training in spawn environments, we first pre-train in the
            # static environments for a couple million time steps. This just
            # provides more opportunities for rewards so makes the initial
            # training easier.
            t_penalty = [2.0e6, 3.5e6]
            t_performance = [1.0e6, 2.0e6]
            level_iterator = SwitchingLevelIterator(
                'random/append-still-easy.yaml',
                'random/append-spawn.yaml',
                t_switch=1.5e6,
                logger=data_logger,
                seed=seed2,
            )
            test_levels = 'benchmarks/v1.0/append-spawn.npz'

        elif env_type == 'prune-spawn':
            t_penalty = [1.5e6, 2.5e6]
            t_performance = [0.5e6, 1.5e6]
            level_iterator = SwitchingLevelIterator(
                'random/prune-still-easy.yaml',
                'random/prune-spawn.yaml',
                t_switch=2.0e6,
                logger=data_logger,
                seed=seed2,
            )
            test_levels = 'benchmarks/v1.0/prune-spawn.npz'

        elif env_type == 'navigate':
            t_penalty = [1.0e6, 2.0e6]
            t_performance = [1.0e6, 2.0e6]  # not actually relevant for navigate
            level_iterator = SafeLifeLevelIterator(
                'random/navigation.yaml', seed=seed2,
            )
            test_levels = 'benchmarks/v1.0/navigation.npz'
        else:
            logging.error("Unexpected environment type '%s'", env_type)

        training_envs = safelife_env_factory(
            data_logger=data_logger, num_envs=16,
            impact_penalty=LinearSchedule(data_logger, t_penalty, [0, penalty]),
            penalty_baseline=args.penalty_baseline,
            min_performance=LinearSchedule(data_logger, t_performance, [0.01, 0.5]),
            level_iterator=level_iterator,
        )
        if args.run_benchmarks:
            bench_level_iterator = SafeLifeLevelIterator(
                args.which_benchmark,
                seed=seed2
            )

            testing_envs = safelife_env_factory(
                data_logger=data_logger, num_envs=20, testing=True,
                level_iterator=bench_level_iterator
                )
        else:
            testing_envs = safelife_env_factory(
                data_logger=data_logger, num_envs=5, testing=True,
                level_iterator=SafeLifeLevelIterator(
                    test_levels, distinct_levels=5, repeat_levels=True)
            )

        if args.algo == 'ppo':
            from training.models import SafeLifePolicyNetwork
            from training.ppo import PPO

            obs_shape = training_envs[0].observation_space.shape
            model = SafeLifePolicyNetwork(obs_shape)
            algo = PPO(
                model,
                training_envs=training_envs,
                testing_envs=testing_envs,
                data_logger=data_logger)

        elif args.algo == 'dqn':
            from training.models import SafeLifeQNetwork
            from training.dqn import DQN

            obs_shape = training_envs[0].observation_space.shape
            train_model = SafeLifeQNetwork(obs_shape)
            target_model = SafeLifeQNetwork(obs_shape)
            algo = DQN(
                train_model, target_model,
                training_envs=training_envs,
                testing_envs=testing_envs,
                data_logger=data_logger)

        elif args.algo == 'wrappedDQN':
            from training.models import SafeLifeQNetwork
            from training.avoidGreen import DQNAvoidGreen

            obs_shape = training_envs[0].observation_space.shape
            train_model = SafeLifeQNetwork(obs_shape)
            target_model = SafeLifeQNetwork(obs_shape)
            algo = DQNAvoidGreen(
                train_model, target_model, 
                training_envs=training_envs,
                testing_envs = testing_envs,
                data_logger=data_logger)
        elif args.algo == 'wrappedPPO':
            from training.models import SafeLifePolicyNetwork
            from training.ppoAvoidGreen import AvoidGreenPPO

            obs_shape = training_envs[0].observation_space.shape
            model = SafeLifePolicyNetwork(obs_shape)
            algo = AvoidGreenPPO(
                model,
                training_envs=training_envs,
                testing_envs=testing_envs,
                data_logger=data_logger)
         

        elif args.algo=='ua':
            from training.models import SafeLifeQNetwork
            from training.uniformAgent import UniformAgent

            obs_shape = training_envs[0].observation_space.shape
            train_model = SafeLifeQNetwork(obs_shape)
            target_model = SafeLifeQNetwork(obs_shape)
            algo = UniformAgent(
                train_model, target_model,
                training_envs=training_envs,
                testing_envs=testing_envs,
                data_logger=data_logger)
        
        elif args.algo=='noop':
            from training.models import SafeLifeQNetwork
            from training.noopAgent import NoOpAgent

            obs_shape = training_envs[0].observation_space.shape
            train_model = SafeLifeQNetwork(obs_shape)
            target_model = SafeLifeQNetwork(obs_shape)
            algo = NoOpAgent(
                train_model, target_model,
                training_envs=training_envs,
                testing_envs=testing_envs,
                data_logger=data_logger)

        elif args.algo=='wrappedUA':
            from training.models import SafeLifeQNetwork
            from training.uniformAgentAvoidGreen import UniformAgentAvoidGreen
            
            obs_shape = training_envs[0].observation_space.shape
            train_model = SafeLifeQNetwork(obs_shape)
            target_model = SafeLifeQNetwork(obs_shape)
            algo = UniformAgentAvoidGreen(
                train_model, target_model,
                training_envs=training_envs,
                testing_envs=testing_envs,
                data_logger=data_logger)
       
        elif args.algo == 'confidencePPO':
            from training.models import SafeLifePolicyNetwork
            from training.ppoConfidence import ConfidencePPO

            obs_shape = training_envs[0].observation_space.shape
            model = SafeLifePolicyNetwork(obs_shape)
            algo = ConfidencePPO(
                model,
                training_envs=training_envs,
                testing_envs=testing_envs,
                data_logger=data_logger)

        elif args.algo == 'confidenceDQN':
            from training.models import SafeLifeQNetwork
            from training.dqnConfidence import ConfidenceDQN

            obs_shape = training_envs[0].observation_space.shape
            train_model = SafeLifeQNetwork(obs_shape)
            target_model = SafeLifeQNetwork(obs_shape)
            algo = ConfidenceDQN(
                train_model, target_model,
                training_envs=training_envs,
                testing_envs=testing_envs,
                data_logger=data_logger)
       


        else:
            logging.error("Unexpected algorithm type '%s'", args.algo)
            raise ValueError("unexpected algorithm type")


        if args.run_benchmarks:
            algo.run_episodes(testing_envs, num_episodes=1000)
        else:
            algo.train(3e7)

except Exception:
    logging.exception("Ran into an unexpected error. Aborting training.")
finally:
    if args.port:
        tb_proc.kill()
    if args.shutdown:
        # Shutdown in 3 minutes.
        # Enough time to recover if it crashed at the start.
        subprocess.run("sudo shutdown +3".split())
        logging.critical("Shutdown commenced. Exiting to bash.")
        subprocess.run(["bash", "-il"])
